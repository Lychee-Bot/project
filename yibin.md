## Actuation

We have thought about two differnt approaches to achieve our goals. Approach 1 leverages existing turtlebot pacakge, but we soon found out a major drawback that prevent us using approach 1, so we switch to approach 2. In Approach 2, we set up our own master server that compute the path based on human modeling constraints.

### Approach 1: Map Update
The first time we thought about "turtelbot avoids pedestrain and goes to a specific location on map" problem, we were confident that there must be online solutions, since the turtlebot is very popular. Indeed, there is one path planning package called ```actionlib``` which takes a stacic map and a goal (inspired by [Learn Turtlebot and ROS](https://github.com/markwsilliman/turtlebot/)). It has a built-in function that will compute the path for turtlebot, and moves the turtlebot to desired location. However, this method doesn't do well in our goal: 

* The map is static. In our case, pedestrain is always moving. A static map doesn't work.
* The computed path may not be our desired path. From our human modeling, we find the optimal threshold of distance turtlebot has to keep from pedestrain. The path generated by ```actionlib``` may not obey this rule.

We then come up with a modifeid solution to use ```actionlib``` package: considering pedestrain as a moving obstacle (circle, with the radius as our distance threshold) [insert picture here, caption: we write a python script to generate pgm map. The ] in the map, and each timesteps we will update the map by moving the circle. The steps are:

1. Create a map of the mocap room, use ```actionlib``` package to generate a path.
2. Add pedestrain as a moving circular obstacle, and update (generate new map) the circle every time-step. Call ```actionlib``` again to update our path
3. Turtlebot moves based on current path command.
4. Repeat step 2 and step 3 untill turtlebot reaches goal location.

We test our approach first by running ```rosrun map_server map_saver``` to create a layour map (pgm image), and then modify the map with image editing tools to add walls (black region in the map). Second, we run the 


### Approach 2: Master Plannar


### Localization
We leverage the Optitrack system to do the localization for turtlebot and pedestrain. Turtlebot will have markers on the top, and pedestrain will wear a cap with markers attach to it. Once the system find turtlebot and cap (pedestrain), it automatically broadcasts to the same network. Note that right now turtlebot will receive the localization data instead of the Ros Computer, so the previous ```mocap_optitrack``` package doesn't work on the turtlebot. Then we use a ros package called ```vrpn_client``` to receive the location message by simply subscribing to ```/vrpn_client_node/turtlebot/pose``` and ```/vrpn_client_node/cap/pose``` to receive turtlebot. [insert image here, maybe] Note 

challenges: Hard to use mocap_optitrack because the ros master is not the same...
 
## Optitrack System
The [Optitrack System](https://optitrack.com/) [pictures of the optitrack room] is an advanced motion capture system. It utilizes infrad camera to localize the opjects by tracking the special IR-reflective markers. It is a high-accuracy and low-latency system that fits best for our project needs. We could read, observe, and analyze the data from the system convinently.

Before the system could read the localtion of object, we stick several special markers to the object we want to track on. Then, we group markes together by different objects as "rigid body." After setting up the rigid bodies, the optitrack system will automatically broadcast the location of the rigid bodies to the network.

We plan to use the Ros Computer in Cory 337 lab to receive location data from Optitrack system, but it actually takes an unexpected long time for us to figure out, as we the system initially doesn't set up quite right. We went through lots of online documentations, official videos, and finally find that we could use the ```mocap_optitrack``` pacakge to receive the data.
